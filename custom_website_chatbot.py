# -*- coding: utf-8 -*-
"""Custom_Website_Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14aeoIsrvTbFNCYekCNshPvQPClb1wnSO
"""

!pip install --force-reinstall -q langchain langchain-community
!pip -q install pypdf
!pip -q install sentence_transformers
!pip install openai
!pip install tiktoken

!pip install tokenizers
!pip install faiss-cpu
!pip -q install unstructured

!pip install numpy==1.26.4
!pip install nltk==3.9.1
!pip install langchain==0.1.20
!pip install langchain-community
!pip install langchain-core
!pip install langchain==0.1.16
!pip install langchain-community==0.0.32
!pip install langchain-core==0.1.31
!pip install langchain langchain-community langchain-text-splitters chromadb
!pip install openai tiktoken
!pip install langchain langchain-community langchain-text-splitters chromadb
!pip install openai tiktoken

import sys
import os
import torch
import textwrap
from langchain_community.document_loaders import UnstructuredURLLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

!pip install langchain-text-splitters

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

#os.environ['OPENAI_API_KEY']=''
os.environ["HUGGINGFACEHUB_API_TOKEN"] =''

URLs=['https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb',
     'https://www.databricks.com/blog/mpt-7b',
     'https://stability.ai/news/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models',
'https://lmsys.org/blog/2023-03-30-vicuna/']

loaders=UnstructuredURLLoader(urls=URLs)
data=loaders.load()

data

len(data)

text_splitter=CharacterTextSplitter(separator='\n',chunk_size=1000,chunk_overlap=200)
text_chunks=text_splitter.split_documents(data)

text_chunks[0]

text_chunks[1]

from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

query_result = embeddings.embed_query("hello world")

len(query_result)

query_result

from langchain_community.vectorstores import FAISS
vectorstore=FAISS.from_documents(text_chunks,embeddings)

from langchain_community.llms import HuggingFacePipeline
from transformers import pipeline

pipe = pipeline(
    "text2text-generation",
    model="google/flan-t5-base"
)

llm = HuggingFacePipeline(pipeline=pipe)

llm.invoke("please provide concise summary of book harry potter")

chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

result=chain({"query":"please provide concise summary of book harry potter"},return_only_outputs=True)

answer = result.get("answer") or result.get("result") or result.get("output_text") or result.get("response")
print(answer)

result=chain({"query":"how does llama 2 outperfroms other models"},return_only_outputs=True)

answer = result.get("answer") or result.get("result") or result.get("output_text") or result.get("response")
print(answer)

result=chain({"query":"tell me about india"},return_only_outputs=True)

answer = result.get("answer") or result.get("result") or result.get("output_text") or result.get("response")
print(answer)

while True:
  query=input(f"Prompt: ")
  if query=='exit':
   print('Exiting')
   sys.exit()
  if query=='':
    continue
  result=chain({'query':query})
  answer = result.get("answer") or result.get("result") or result.get("output_text") or result.get("response")
  print(f"Answer: {answer} ")

from google.colab import notebook
notebook.exportNotebook('/content/Custom_Website_Chatbot.ipynb')

# Commented out IPython magic to ensure Python compatibility.
# %%javascript
# (async function() {
#   await google.colab.notebook.save();
#   console.log("Notebook saved!");
# })();
#

!find /content -maxdepth 2 -type f -name "*.ipynb"

import json

path = "/content/Custom_Website_Chatbot.ipynb"   # change if needed

with open(path) as f:
    nb = json.load(f)

# Remove widget metadata safely
if "widgets" in nb.get("metadata", {}):
    del nb["metadata"]["widgets"]

with open(path, "w") as f:
    json.dump(nb, f)

