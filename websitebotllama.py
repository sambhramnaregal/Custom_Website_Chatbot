# -*- coding: utf-8 -*-
"""WebsiteBotLlama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-aNQTAQ869Sk2NzjFuYqoAKliRw3sJYJ
"""

!pip install -q langchain
!pip install -q bitsandbytes accelerate transformers
!pip install -q datasets loralib sentencepiece
!pip install -q pypdf
!pip install -q sentence-transformers

!pip install -q unstructured

!pip install tokenizers

!pip install xformers

!pip install pinecone-client

!pip install -q langchain langchain-community langchain-core langchain-text-splitters

!pip uninstall -y pinecone-client
!pip install -q pinecone

# Document loaders
from langchain_community.document_loaders import UnstructuredURLLoader

# Text Splitter (NEW LOCATION)
from langchain_text_splitters import CharacterTextSplitter

# Embeddings
from langchain_community.embeddings import HuggingFaceEmbeddings

# Vectorstore (Pinecone)
from langchain_community.vectorstores import Pinecone

# LLM Pipeline (HuggingFace)
from langchain_community.llms import HuggingFacePipeline

# Transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# HuggingFace Hub
from huggingface_hub import notebook_login

# Utilities
import textwrap
import sys
import os
import torch

# Pinecone
import pinecone

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

URLs=['https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb',
      'https://www.databricks.com/blog/mpt-7b',
      'https://stability.ai/news/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models',
      'https://lmsys.org/blog/2023-03-30-vicuna/']

loaders=UnstructuredURLLoader(URLs)
data=loaders.load()

data

len(data)

type(data)

text_splitter=CharacterTextSplitter(separator='\n',chunk_size=1000,chunk_overlap=200)

text_chunks=text_splitter.split_documents(data)

text_chunks[0]

text_chunks

embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')

embeddings

query_result=embeddings.embed_query("hello  how are oyu world")

len(query_result)

import os
PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY','pcsk_2JJQtW_S2RfAAX7e3YyBCK3bCz491JutEQEyfk3vhqr3pXpMbm2F441EuYtaT213a82qM5')
PINECONE_ENV=os.environ.get('us-east-1')

!pip uninstall -y pinecone-client pinecone
!pip install pinecone
!pip install langchain-pinecone

from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore
pc = Pinecone(api_key=PINECONE_API_KEY)
index_name = "llama"
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=384,
        metric="cosine",
        spec=ServerlessSpec(
            cloud="gcp",
            region="us-central1"
        )
    )

from pinecone import Pinecone, ServerlessSpec

pc = Pinecone(api_key=PINECONE_API_KEY)
index_name = "llama"

index = pc.Index(index_name)

vector = embeddings.embed_query("test insert")

index.upsert([
    {
        "id": "test-1",
        "values": vector,
        "metadata": {"text": "test insert"}
    }
])

vectorstore.add_documents(text_chunks)

vectorstore = PineconeVectorStore(
    index=index,
    embedding=embeddings
)

vectorstore.add_documents(text_chunks)

index.describe_index_stats()

notebook_login()

model="meta-llama/Llama-2-7b-chat-hf"

tokenizer=AutoTokenizer.from_pretrained(model,use_auth_token=True,)
model=AutoModelForCausalLM.from_pretrained(model,
                                           device_map='auto',
                                           torch_dtype=torch.float16,
                                           use_auth_token=True,
                                           load_in_8bit=True,)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    max_new_tokens=512,
    do_sample=True,
    top_k=30,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id
)

llm = HuggingFacePipeline(
    pipeline=pipe,
    model_kwargs={'temperature': 0}
)

response = llm.invoke(
    "Please provide a concise summary of the book Harry Potter."
)

print(response)

query="How good is Vicuna"
docs=vectorstore.similarity_search(query,k=3)

docs

!pip install "langchain==1.1.0"

response1 = llm.invoke("how good is vicuna.")
response2 = llm.invoke("how does llama 2 outperforms other models.")
response3 = llm.invoke("what is stableLM.")

print(response1)

print(response2)

print(response3)

